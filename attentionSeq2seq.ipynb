{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考文献】\n",
    "【核】\n",
    "https://qiita.com/m__k/items/646044788c5f94eadc8d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 事前にGoogle Driveをマウントして以下の場所にdate.txtを格納しておく\n",
    "file_path = \"date.txt\"\n",
    "\n",
    "input_date = [] # 変換前の日付データ\n",
    "output_date = [] # 変換後の日付データ\n",
    "\n",
    "# date.txtを1行ずつ読み込んで変換前と変換後に分割して、inputとoutputで分ける\n",
    "with open(file_path, \"r\") as f:\n",
    "  date_list = f.readlines()\n",
    "  for date in date_list:\n",
    "    date = date[:-1]\n",
    "    input_date.append(date.split(\"_\")[0])\n",
    "    output_date.append(\"_\" + date.split(\"_\")[1])\n",
    "\n",
    "# inputとoutputの系列の長さを取得\n",
    "# すべて長さが同じなので、0番目の要素でlenを取ってます\n",
    "input_len = len(input_date[0]) # 29\n",
    "output_len = len(output_date[0]) # 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date.txtで登場するすべての文字にIDを割り当てる\n",
    "char2id = {}\n",
    "for input_chars, output_chars in zip(input_date, output_date):\n",
    "  for c in input_chars:\n",
    "    if not c in char2id:\n",
    "      char2id[c] = len(char2id)\n",
    "  for c in output_chars:\n",
    "    if not c in char2id:\n",
    "      char2id[c] = len(char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [] # ID化された変換前日付データ\n",
    "output_data = [] # ID化された変換後日付データ\n",
    "for input_chars, output_chars in zip(input_date, output_date):\n",
    "  input_data.append([char2id[c] for c in input_chars])\n",
    "  output_data.append([char2id[c] for c in output_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:3でtrainとtestに分ける\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, train_size= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをバッチ化するための関数を定義\n",
    "def train2batch(input_data, output_data, batch_size=100):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    input_shuffle, output_shuffle = shuffle(input_data, output_data)\n",
    "    for i in range(0, len(input_data), batch_size):\n",
    "      input_batch.append(input_shuffle[i:i+batch_size])\n",
    "      output_batch.append(output_shuffle[i:i+batch_size])\n",
    "    return input_batch, output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 諸々のパラメータなど\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "BATCH_NUM = 100\n",
    "vocab_size = len(char2id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Encoderクラス\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[\" \"])\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        embedding = self.word_embeddings(sequence)\n",
    "        # hsが各系列のGRUの隠れ層のベクトル\n",
    "        # Attentionされる要素\n",
    "        hs, h = self.gru(embedding)\n",
    "        return hs, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Decoderクラス\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[\" \"])\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # hidden_dim*2としているのは、各系列のGRUの隠れ層とAttention層で計算したコンテキストベクトルをtorch.catでつなぎ合わせることで長さが２倍になるため\n",
    "        self.hidden2linear = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        # 列方向を確率変換したいのでdim=1\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, sequence, hs, h):\n",
    "        embedding = self.word_embeddings(sequence)\n",
    "        output, state = self.gru(embedding, h)\n",
    "\n",
    "       # Attention層\n",
    "       # hs.size() = ([100, 29, 128])\n",
    "       # output.size() = ([100, 10, 128])\n",
    "\n",
    "       # bmmを使ってEncoder側の出力(hs)とDecoder側の出力(output)をbatchごとまとめて行列計算するために、Decoder側のoutputをbatchを固定して転置行列を取る\n",
    "        t_output = torch.transpose(output, 1, 2) # t_output.size() = ([100, 128, 10])\n",
    "\n",
    "        # bmmでバッチも考慮してまとめて行列計算\n",
    "        s = torch.bmm(hs, t_output) # s.size() = ([100, 29, 10])\n",
    "\n",
    "        # 列方向(dim=1)でsoftmaxをとって確率表現に変換\n",
    "        # この値を後のAttentionの可視化などにも使うため、returnで返しておく\n",
    "        attention_weight = self.softmax(s) # attention_weight.size() = ([100, 29, 10])\n",
    "\n",
    "        # コンテキストベクトルをまとめるために入れ物を用意\n",
    "        c = torch.zeros(self.batch_size, 1, self.hidden_dim, device=device) # c.size() = ([100, 1, 128])\n",
    "\n",
    "        # 各DecoderのGRU層に対するコンテキストベクトルをまとめて計算する方法がわからなかったので、\n",
    "        # 各層（Decoder側のGRU層は生成文字列が10文字なので10個ある）におけるattention weightを取り出してforループ内でコンテキストベクトルを１つずつ作成する\n",
    "        # バッチ方向はまとめて計算できたのでバッチはそのまま\n",
    "        for i in range(attention_weight.size()[2]): # 10回ループ\n",
    "\n",
    "          # attention_weight[:,:,i].size() = ([100, 29])\n",
    "          # i番目のGRU層に対するattention weightを取り出すが、テンソルのサイズをhsと揃えるためにunsqueezeする\n",
    "          unsq_weight = attention_weight[:,:,i].unsqueeze(2) # unsq_weight.size() = ([100, 29, 1])\n",
    "\n",
    "          # hsの各ベクトルをattention weightで重み付けする\n",
    "          weighted_hs = hs * unsq_weight # weighted_hs.size() = ([100, 29, 128])\n",
    "\n",
    "          # attention weightで重み付けされた各hsのベクトルをすべて足し合わせてコンテキストベクトルを作成\n",
    "          weight_sum = torch.sum(weighted_hs, axis=1).unsqueeze(1) # weight_sum.size() = ([100, 1, 128])\n",
    "\n",
    "          c = torch.cat([c, weight_sum], dim=1) # c.size() = ([100, i, 128])\n",
    "          \n",
    "        # 箱として用意したzero要素が残っているのでスライスして削除\n",
    "        c = c[:,1:,:]\n",
    "\n",
    "        output = torch.cat([output, c], dim=2) # output.size() = ([100, 10, 256])\n",
    "        output = self.hidden2linear(output)\n",
    "        return output, state, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "attn_decoder = AttentionDecoder(vocab_size, embedding_dim, hidden_dim, BATCH_NUM).to(device)\n",
    "\n",
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "attn_decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...\n",
      "Epoch 1: 1637.91\n",
      "Epoch 2: 74.50\n",
      "Epoch 3: 10.17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mセル10 を c:\\Users\\kenta\\OneDrive\\ドキュメント\\study_party\\Atention\\attentionSeq2seq.ipynb\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=17'>18</a>\u001b[0m output_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(output_batch[i], device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=19'>20</a>\u001b[0m \u001b[39m# Encoderの順伝搬\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=20'>21</a>\u001b[0m hs, h \u001b[39m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=22'>23</a>\u001b[0m \u001b[39m# Attention Decoderのインプット\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=23'>24</a>\u001b[0m source \u001b[39m=\u001b[39m output_tensor[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kenta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mセル10 を c:\\Users\\kenta\\OneDrive\\ドキュメント\\study_party\\Atention\\attentionSeq2seq.ipynb\u001b[0m in \u001b[0;36mEncoder.forward\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=20'>21</a>\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(sequence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=21'>22</a>\u001b[0m \u001b[39m# hsが各系列のGRUの隠れ層のベクトル\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=22'>23</a>\u001b[0m \u001b[39m# Attentionされる要素\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=23'>24</a>\u001b[0m hs, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(embedding)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000010?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hs, h\n",
      "File \u001b[1;32mc:\\Users\\kenta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kenta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    949\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 950\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    951\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    954\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_NUM=100\n",
    "EPOCH_NUM = 100\n",
    "\n",
    "all_losses = []\n",
    "print(\"training ...\")\n",
    "for epoch in range(1, EPOCH_NUM+1):\n",
    "    epoch_loss = 0\n",
    "    # データをミニバッチに分ける\n",
    "    input_batch, output_batch = train2batch(train_x, train_y, batch_size=BATCH_NUM)\n",
    "    for i in range(len(input_batch)):\n",
    "\n",
    "        # 勾配の初期化\n",
    "        encoder_optimizer.zero_grad()\n",
    "        attn_decoder_optimizer.zero_grad()\n",
    "\n",
    "        # データをテンソルに変換\n",
    "        input_tensor = torch.tensor(input_batch[i], device=device)\n",
    "        output_tensor = torch.tensor(output_batch[i], device=device)\n",
    "\n",
    "        # Encoderの順伝搬\n",
    "        hs, h = encoder(input_tensor)\n",
    "\n",
    "        # Attention Decoderのインプット\n",
    "        source = output_tensor[:, :-1]\n",
    "\n",
    "        # Attention Decoderの正解データ\n",
    "        target = output_tensor[:, 1:]\n",
    "\n",
    "        loss = 0\n",
    "        decoder_output, _, attention_weight= attn_decoder(source, hs, h)\n",
    "        for j in range(decoder_output.size()[1]):\n",
    "            loss += criterion(decoder_output[:, j, :], target[:, j])\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 誤差逆伝播\n",
    "        loss.backward()\n",
    "\n",
    "        # パラメータ更新\n",
    "        encoder_optimizer.step()\n",
    "        attn_decoder_optimizer.step()\n",
    "\n",
    "    # 損失を表示\n",
    "    print(\"Epoch %d: %.2f\" % (epoch, epoch_loss))\n",
    "    all_losses.append(epoch_loss)\n",
    "    if epoch_loss < 0.1: break\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル11 を c:\\Users\\kenta\\OneDrive\\ドキュメント\\study_party\\Atention\\attentionSeq2seq.ipynb\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000011?line=2'>3</a>\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000011?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mstyle\u001b[39m.\u001b[39muse(\u001b[39m'\u001b[39m\u001b[39mggplot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000011?line=4'>5</a>\u001b[0m ax\u001b[39m.\u001b[39mplot(all_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000011?line=5'>6</a>\u001b[0m \u001b[39m#ax.plot(val_losses, label='val loss')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kenta/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/study_party/Atention/attentionSeq2seq.ipynb#ch0000011?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.style.use('ggplot')\n",
    "ax.plot(all_losses, label='train loss')\n",
    "#ax.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(\"/public/tsukaue/study_party/Attention/loss of attentionSeq2seq.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3645da56cb814aa032b2cac3a7394756acee5934a6fcdb3f4bf73fac323e5936"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
